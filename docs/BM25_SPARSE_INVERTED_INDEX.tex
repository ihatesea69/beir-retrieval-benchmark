\section{BM25 Index, Sparse Index \& Inverted Index - Detailed Explanation}

\subsection{ Quick Overview}

\begin{tabular}{|l|l|l|}
  \hline
  Concept & What is it? & Example \\
  \hline
  \textbf{BM25} & Ranking algorithm & Formula to score how well a doc matches a query \\
  \textbf{Sparse Index} & Data structure & Dictionary storing which words are in which docs \\
  \textbf{Inverted Index} & The actual index & Map from words → documents (opposite of normal) \\
  \hline
\end{tabular}

\subsection{1⃣ What is BM25?}

\subsubsection{Definition}

\textbf{BM25} = \textbf{Best Matching 25} = A probabilistic ranking function that scores how relevant a document is to a query.

Think of it as:

\begin{itemize}
  \item \textbf{Formula}: Math equation that assigns a relevance score
  \item \textbf{Ranking system}: Determines order of search results
  \item \textbf{Not storage}: It's about HOW to score, not WHERE to store
\end{itemize}

\subsubsection{The BM25 Formula}

\begin{lstlisting}
score(D, Q) = Σ(i=1 to n) IDF(qi) × [f(qi, D) × (k1 + 1)] / [f(qi, D) + k1 × (1 - b + b × |D|/avgdl)]

Where:
- D = Document
- Q = Query with n terms
- qi = i-th term in query
- f(qi, D) = frequency of term qi in document D
- |D| = length of document D
- avgdl = average document length in collection
- IDF(qi) = Inverse Document Frequency of term qi
- k1 = term saturation parameter (typically 1.2-2.0)
- b = length normalization parameter (typically 0.75)
\end{lstlisting}

\subsubsection{Breaking Down the Formula}

\paragraph{Part 1: IDF (Inverse Document Frequency)}

\begin{lstlisting}
IDF(qi) = log((N - df(qi) + 0.5) / (df(qi) + 0.5))

Where:
- N = total number of documents
- df(qi) = number of documents containing term qi

Example with NFCorpus:
N = 3,633 documents

Word "cancer":
- Appears in 790 documents
- IDF = log((3633 - 790 + 0.5) / (790 + 0.5))
- IDF = log(2843.5 / 790.5) = log(3.596) ≈ 1.28

Word "the":
- Appears in ~3600 documents (very common)
- IDF = log((3633 - 3600 + 0.5) / (3600 + 0.5))
- IDF = log(33.5 / 3600.5) = log(0.0093) ≈ -4.68 (very negative!)

Word "methylmercury":
- Appears in only 5 documents (rare)
- IDF = log((3633 - 5 + 0.5) / (5 + 0.5))
- IDF = log(3628.5 / 5.5) = log(659.7) ≈ 6.49 (very high!)

Key insight: Rare words = higher IDF = more important for ranking
\end{lstlisting}

\paragraph{Part 2: Term Frequency Saturation}

\begin{lstlisting}
[f(qi, D) × (k1 + 1)] / [f(qi, D) + k1 × (1 - b + b × |D|/avgdl)]

With k1 = 1.5 (typical value):

If "cancer" appears 1 time in document:
score_component = (1 × 2.5) / (1 + 1.5) = 2.5 / 2.5 = 1.0

If "cancer" appears 5 times in document:
score_component = (5 × 2.5) / (5 + 1.5) = 12.5 / 6.5 ≈ 1.92

If "cancer" appears 100 times in document:
score_component = (100 × 2.5) / (100 + 1.5) = 250 / 101.5 ≈ 2.46

Key insight: More mentions helps, but diminishing returns (saturation)
Not linear! 100 mentions doesn't give 100x better score
\end{lstlisting}

\paragraph{Part 3: Length Normalization}

\begin{lstlisting}
(1 - b + b × |D|/avgdl)

With b = 0.75 (typical value) and avgdl = 250 words:

Short document (100 words):
normalize = (1 - 0.75 + 0.75 × 100/250) = 0.25 + 0.30 = 0.55
→ Shorter docs get slightly boosted

Long document (500 words):
normalize = (1 - 0.75 + 0.75 × 500/250) = 0.25 + 1.50 = 1.75
→ Longer docs get penalized

Key insight: Prevents long documents from always winning
\end{lstlisting}

\subsubsection{Simple BM25 Example}

\begin{lstlisting}
Query: "cancer treatment"
Corpus: 3 documents

Document 1:
Title: "Statin Use and Breast Cancer Survival"
Text: "...cancer...breast cancer...treatment..."
Length: 250 words

BM25 Calculation:
term1 = "cancer" (appears 2 times)
  IDF("cancer") = 1.28
  freq_score = (2 × 2.5) / (2 + 1.5 × (1 - 0.75 + 0.75 × 250/250))
             = (5.0) / (2 + 1.5 × 1.0)
             = 5.0 / 3.5 = 1.43
  contribution1 = 1.28 × 1.43 = 1.83

term2 = "treatment" (appears 1 time)
  IDF("treatment") = 0.95
  freq_score = 1.43
  contribution2 = 0.95 × 1.43 = 1.36

Total BM25 score = 1.83 + 1.36 = 3.19

---

Document 2:
Title: "Diabetes Prevention Through Lifestyle"
Text: "...prevention...treatment..."
Length: 200 words

BM25 Calculation:
term1 = "cancer" (appears 0 times)
  contribution1 = 0

term2 = "treatment" (appears 1 time)
  IDF("treatment") = 0.95
  freq_score = ...
  contribution2 = 0.95 × 1.43 = 1.36

Total BM25 score = 0 + 1.36 = 1.36

---

Ranking:
1st: Document 1 (score 3.19)
2nd: Document 2 (score 1.36)
\end{lstlisting}

\subsection{2⃣ What is Sparse Index?}

\subsubsection{Definition}

\textbf{Sparse Index} = Index structure where data is \textbf{sparse} (mostly empty/zeros)

Contrast:

\begin{itemize}
  \item \textbf{Dense}: Most positions have values (dense representation)
  \item \textbf{Sparse}: Most positions are zero/empty (sparse representation)
\end{itemize}

\subsubsection{Why "Sparse"?}

\begin{lstlisting}
For Document: "Statin Use and Breast Cancer Survival"

Vocabulary Size: 100,000 words (entire NFCorpus)

Representation Options:

DENSE VECTOR (like in Deep Learning):
[0.45, 0.12, 0.89, 0.03, 0.00, 0.12, ..., 0.05] → 100,000 floats
Size: 100,000 × 4 bytes = 400 KB per document
Total for 3,633 docs: 1.45 GB

SPARSE VECTOR (like BM25):
{
  'statin': 2,
  'use': 1,
  'breast': 1,
  'cancer': 1,
  'survival': 1
}
Size: ~40 bytes (5 words only!)
Total for 3,633 docs: ~150 KB

→ Sparse is 100x smaller!
\end{lstlisting}

\subsubsection{Sparse Index Structure}

\begin{lstlisting}
BM25 Sparse Index for NFCorpus:
═════════════════════════════════

Per Document: Store only the words that appear
┌─ Document MED-10:
│  {
│    'statin': 2,        // appears 2 times
│    'breast': 1,
│    'cancer': 2,
│    'survival': 1,
│    'study': 1,
│    'death': 3,
│    'patient': 2,
│    'diagnosis': 1
│  }
│
└─ Document MED-14:
   {
     'statin': 1,
     'cancer': 1,
     'diagnosis': 1,
     'mortality': 2,
     'study': 1,
     'patient': 1
   }

Advantages:
✅ Small memory footprint
✅ Fast to store & retrieve
✅ Only tracks relevant words

Disadvantages:
❌ Can't capture synonyms ("cancer" vs "tumor")
❌ Misses semantic relationships
❌ Lexical mismatch problem
\end{lstlisting}

\subsection{3⃣ What is Inverted Index?}

\subsubsection{Definition}

\textbf{Inverted Index} = Map from \textbf{words → documents} (opposite of normal)

\textbf{Normal Index} (Forward Index):

\begin{lstlisting}
Document → Words
MED-10 → ["statin", "breast", "cancer", "survival", ...]
MED-14 → ["statin", "cancer", "diagnosis", "mortality", ...]
\end{lstlisting}

\textbf{Inverted Index} (What BM25 uses):

\begin{lstlisting}
Word → Documents (with positions/frequencies)
"statin" → [MED-10 (freq:2, pos:1), MED-14 (freq:1, pos:5), ...]
"cancer" → [MED-10 (freq:2, pos:8), MED-14 (freq:1, pos:12), ...]
"breast" → [MED-10 (freq:1, pos:7), ...]
\end{lstlisting}

\subsubsection{Why "Inverted"?}

\begin{lstlisting}
Query: "cancer statin"

Normal Index (Forward):
Need to check EVERY document:
- Does MED-1 contain "cancer"? Check full text... No
- Does MED-2 contain "cancer"? Check full text... No
- ...
- Does MED-10 contain "cancer"? Check full text... Yes!
→ Slow! O(n) where n = total documents

Inverted Index:
Look up words directly:
- "cancer" → [MED-10, MED-14, MED-118, ...] ✅ Direct access!
- "statin" → [MED-10, MED-14, ...] ✅ Direct access!
- Intersection → [MED-10, MED-14] (docs with both words)
→ Fast! O(k) where k = documents containing the words
\end{lstlisting}

\subsubsection{Inverted Index Structure}

\begin{lstlisting}
Inverted Index for NFCorpus:
═════════════════════════════

Word → Postings List

"statin":
  ├─ MED-10: {freq: 2, positions: [1, 45], tf-idf: 1.23}
  ├─ MED-14: {freq: 1, positions: [5], tf-idf: 0.95}
  ├─ MED-301: {freq: 3, positions: [2, 18, 76], tf-idf: 1.45}
  └─ ... (appears in 50+ documents)

"cancer":
  ├─ MED-10: {freq: 2, positions: [8, 15], tf-idf: 2.13}
  ├─ MED-14: {freq: 1, positions: [12], tf-idf: 1.64}
  ├─ MED-666: {freq: 5, positions: [3, 11, 22, 34, 50], tf-idf: 2.87}
  └─ ... (appears in 790+ documents)

"zebra":
  ├─ (empty - doesn't appear in medical corpus)

Key Features:
✅ Posting list: All docs containing the word
✅ Frequency: How many times word appears
✅ Positions: Where in document word appears
✅ TF-IDF: Pre-computed relevance weight
\end{lstlisting}

\subsubsection{How Inverted Index is Built}

\begin{lstlisting}
Building Process:

Step 1: Tokenize all documents
    MED-10: "Statin Use and Breast Cancer Survival..."
    → ["statin", "use", "and", "breast", "cancer", "survival", ...]

Step 2: Create word-position pairs
    ("statin", MED-10, pos=1, freq=1)
    ("use", MED-10, pos=2, freq=1)
    ("breast", MED-10, pos=4, freq=1)
    ("cancer", MED-10, pos=5, freq=2)  // appears twice in doc
    ...

Step 3: Sort by word
    "and" → [(MED-10, pos=3), (MED-14, pos=2), ...]
    "breast" → [(MED-10, pos=4), (MED-666, pos=7), ...]
    "cancer" → [(MED-10, pos=5), (MED-14, pos=12), ...]
    "statin" → [(MED-10, pos=1), (MED-14, pos=5), ...]
    ...

Step 4: Compress & optimize
    Store only necessary info for retrieval
    Build B-tree or hash table for O(log n) lookup

Result: Inverted Index ready for searching!
\end{lstlisting}

\subsection{4⃣ BM25 + Sparse Index + Inverted Index Together}

\subsubsection{How They Work Together}

\begin{lstlisting}
BM25 Retrieval Pipeline:
════════════════════════

1. User Query: "cancer treatment"
   ↓

2. Tokenize Query:
   ["cancer", "treatment"]
   ↓

3. Look up in Inverted Index:
   ├─ "cancer" → [MED-10, MED-14, MED-118, ..., MED-666] (790 docs)
   └─ "treatment" → [MED-5, MED-10, MED-20, ...] (420 docs)
   ↓

4. Get Candidate Documents:
   Intersection: [MED-10, MED-14, ...] (docs with BOTH words)
   Union: [MED-10, MED-14, MED-5, ..., MED-666] (docs with ANY word)
   ↓

5. For Each Candidate Document, Calculate BM25 Score:
   
   For MED-10:
   ├─ Retrieve sparse vector: {cancer: 2, treatment: 1, ...}
   ├─ Calculate IDF("cancer") = 1.28
   ├─ Calculate IDF("treatment") = 0.95
   ├─ Apply BM25 formula with these values
   └─ Score = 3.19
   
   For MED-14:
   ├─ Retrieve sparse vector: {cancer: 1, treatment: 0, ...}
   ├─ Calculate BM25 formula
   └─ Score = 2.45
   
   ... (for all candidates)
   ↓

6. Sort by BM25 Score:
   1. MED-10: 3.19 ✅ ("cancer" + "treatment")
   2. MED-666: 2.87 ✅ ("cancer" + some treatment mention)
   3. MED-14: 2.45 ✅ (both words present)
   ...
   ↓

7. Return Top-K Results:
   Query: "cancer treatment"
   Results:
   [1] MED-10 - "Statin Use and Breast Cancer Survival" (score: 3.19)
   [2] MED-666 - "Surgery for Breast Cancer Treatment" (score: 2.87)
   [3] MED-14 - "Statin use after cancer diagnosis" (score: 2.45)
\end{lstlisting}

\subsubsection{Memory Representation During Search}

\begin{lstlisting}
In-Memory BM25 Index (from BM25Retriever):

BM25_INDEX = {
  'nodes': [
    Node(id='MED-10', text='...', metadata={...}),
    Node(id='MED-14', text='...', metadata={...}),
    ...
  ],
  'corpus_tokens': [
    ['statin', 'breast', 'cancer', 'survival', ...],  # MED-10
    ['statin', 'cancer', 'diagnosis', ...],            # MED-14
    ...
  ],
  'idf_dict': {
    'cancer': 1.28,
    'statin': 1.62,
    'breast': 2.15,
    'treatment': 0.95,
    'the': -4.68,
    ...
  },
  'k1': 1.5,
  'b': 0.75,
  'avgdl': 250
}

During Query:
✅ Look up words in idf_dict
✅ Get term frequencies from corpus_tokens
✅ Apply BM25 formula
✅ Sort & return results
\end{lstlisting}

\subsection{5⃣ Sparse vs Dense Comparison}

\begin{lstlisting}
BM25 (Sparse)              Dense (Semantic)
══════════════════════════════════════════════════════════

Index Type:
├─ Inverted Index           ├─ Vector Index (HNSW/IVF)
├─ Sparse: {word: freq}     └─ Dense: [0.45, 0.23, ..., 0.89]

Memory:
├─ 40 bytes per doc         ├─ 1,536 bytes per doc (384 dims)
├─ 150 KB for 3,633 docs    └─ 5.6 MB for 3,633 docs
└─ 38x smaller!             └─ Continuous vectors

Storage:
├─ RAM (LlamaIndex)         ├─ PostgreSQL (persistent)
├─ Non-persistent           └─ Survives restart

Query Processing:
├─ "cancer treatment"       ├─ "cancer treatment"
├─ Look up words directly   ├─ Generate embedding: [0.45, ..., 0.12]
├─ Fast (O(k) lookups)      ├─ Cosine similarity to all docs
└─ Exact matching           └─ Semantic similarity

Strengths:
├─ ✅ Exact keyword match    ├─ ✅ Synonym handling
├─ ✅ Fast computation      ├─ ✅ Semantic understanding
├─ ✅ Interpretable         └─ ✅ "cancer" matches "tumor"
└─ ✅ Efficient storage

Weaknesses:
├─ ❌ Misses synonyms       ├─ ❌ Slower computation
├─ ❌ Lexical mismatch      ├─ ❌ High memory usage
├─ ❌ "cancer" ≠ "tumor"    ├─ ❌ May retrieve unrelated docs
└─ ❌ No semantic link      └─ ❌ "semantic drift"
\end{lstlisting}

\subsection{6⃣ Real-World Example with BM25}

\subsubsection{Example: Query "Statin breast cancer"}

\begin{lstlisting}
Step 1: Tokenize
Query tokens: ["statin", "breast", "cancer"]

Step 2: Inverted Index Lookup
"statin" → [MED-10, MED-14, MED-301, ...]  (50 documents)
"breast" → [MED-10, MED-666, MED-667, ...]  (150 documents)
"cancer" → [MED-10, MED-14, ..., MED-666]  (790 documents)

Step 3: Get Candidates
Docs containing ALL terms: [MED-10, MED-14, MED-301, ...]
Total candidates: ~30 documents

Step 4: Calculate BM25 for Each Candidate

Document: MED-10 "Statin Use and Breast Cancer Survival"
- Length: 1,600 chars ≈ 250 words
- "statin": appears 2 times
  IDF = 1.62
  BM25_statin = 1.62 × [(2×2.5)/(2+1.5×(1-0.75+0.75×250/250))]
              = 1.62 × [5.0/3.5] = 1.62 × 1.43 = 2.32

- "breast": appears 3 times
  IDF = 2.15
  BM25_breast = 2.15 × [(3×2.5)/(3+1.5×1)] = 2.15 × 2.14 = 4.60

- "cancer": appears 4 times
  IDF = 1.28
  BM25_cancer = 1.28 × [(4×2.5)/(4+1.5×1)] = 1.28 × 2.22 = 2.84

Total BM25 = 2.32 + 4.60 + 2.84 = 9.76 ⭐ (HIGH SCORE)

---

Document: MED-14 "Statin use after diagnosis of breast cancer"
- Length: 1,200 chars ≈ 200 words
- "statin": appears 1 time → BM25_statin = 1.62 × 1.43 = 2.32
- "breast": appears 1 time → BM25_breast = 2.15 × 1.43 = 3.07
- "cancer": appears 1 time → BM25_cancer = 1.28 × 1.43 = 1.83

Total BM25 = 2.32 + 3.07 + 1.83 = 7.22 ✅ (GOOD SCORE)

---

Document: MED-666 "Surgery for Breast Cancer Treatment"
- "statin": 0 times → BM25_statin = 0
- "breast": 2 times → BM25_breast = 2.15 × 1.92 = 4.13
- "cancer": 2 times → BM25_cancer = 1.28 × 1.92 = 2.46

Total BM25 = 0 + 4.13 + 2.46 = 6.59 (LOWER - missing "statin")

---

Final Ranking:
1. MED-10: 9.76 ⭐ (has all 3 terms, high frequency)
2. MED-14: 7.22 ✅ (has all 3 terms, lower frequency)
3. MED-666: 6.59   (missing "statin")
4. ...
\end{lstlisting}

\subsection{7⃣ Key Takeaways}

\subsubsection{BM25}

\begin{itemize}
  \item \textbf{What}: Ranking formula (algorithm)
  \item \textbf{Purpose}: Score how well documents match queries
  \item \textbf{Not}: Storage mechanism or index type
  \item \textbf{Formula}: Considers IDF, term frequency, document length
\end{itemize}

\subsubsection{Sparse Index}

\begin{itemize}
  \item \textbf{What}: Index representation (mostly zeros)
  \item \textbf{Advantage}: Memory efficient
  \item \textbf{Used by}: BM25, traditional IR systems
  \item \textbf{Limitation}: Only tracks words that appear
\end{itemize}

\subsubsection{Inverted Index}

\begin{itemize}
  \item \textbf{What}: Data structure (word → documents mapping)
  \item \textbf{Purpose}: Fast lookup of which documents contain words
  \item \textbf{Speed}: O(log n) or O(k) vs O(n)
  \item \textbf{Used by}: Every full-text search engine (Google, Elasticsearch, etc.)
\end{itemize}

\subsubsection{In Your Project}

\begin{lstlisting}
LlamaIndex BM25Retriever:
├─ Builds Inverted Index from documents
├─ Stores Sparse vectors (word frequencies)
├─ Uses BM25 formula for ranking
└─ Returns top-k results
\end{lstlisting}

\subsection{ Visual Summary}

\begin{lstlisting}
Query: "cancer treatment"
          ↓
    ┌─────┴─────┐
    ↓           ↓
SPARSE      DENSE
INDEX       EMBEDDING
    │           │
    ├─Inverted  ├─Vector
    │ Index     │ Index
    │           │
    ├─"cancer"  ├─Generate
    │ → docs    │ embedding
    │           │
    └─BM25      └─Cosine
      ranking     similarity

Result: Ranked documents
\end{lstlisting}

\subsection{ Remember}

\textbf{Sparse = Few non-zero values (efficient)}
\textbf{Inverted = Word-to-docs mapping (fast lookup)}
\textbf{BM25 = Ranking formula (relevance scoring)}

All three work together to make full-text search fast and effective!