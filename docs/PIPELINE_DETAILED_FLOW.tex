\section{Pipeline Detailed Flow: Input â†’ Processing â†’ Output}

\subsection{ Overview}

Document nÃ y giáº£i thÃ­ch chi tiáº¿t \textbf{Input}, \textbf{Processing}, vÃ  \textbf{Output} cá»§a tá»«ng phase trong há»‡ thá»‘ng so sÃ¡nh BM25 vs Dense vs Hybrid retrieval.

\subsection{ PHASE 1: DATA LAYER - BeIR Data Loading}

\subsubsection{ Module: \texttt{data\_loader.py} â†’ \texttt{BeirDataLoader}}

\paragraph{ \textbf{INPUT}}

\begin{lstlisting}[language=python]
# Method: load_dataset('nfcorpus')
dataset_name = 'nfcorpus'  # String: tÃªn dataset

# Source files (downloaded from BeIR):
# - corpus.jsonl (3,634 lines)
# - queries.jsonl (3,238 lines)  
# - qrels/dev.tsv (11,387 lines)
\end{lstlisting}

\textbf{Example Raw Input (corpus.jsonl line 1)}:

\begin{lstlisting}[language=json]
{
  "_id": "MED-10",
  "title": "Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland",
  "text": "Recent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality, could delay or prevent breast cancer recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of breast cancer death among statin users in a population-based cohort of breast cancer patients. The study cohort included all newly diagnosed breast cancer patients in Finland during 1995â€“2003 (31,236 cases), identified from the Finnish Cancer Registry. Information on statin use before and after the diagnosis was obtained from a national prescription database. We used the Cox proportional hazards regression method to estimate mortality among statin users with statin use as time-dependent variable. A total of 4,151 participants had used statins. During the median follow-up of 3.25 years after the diagnosis (range 0.08â€“9.0 years) 6,011 participants died, of which 3,619 (60.2%) was due to breast cancer. After adjustment for age, tumor characteristics, and treatment selection, both post-diagnostic and pre-diagnostic statin use were associated with lowered risk of breast cancer death (HR 0.46, 95% CI 0.38â€“0.55 and HR 0.54, 95% CI 0.44â€“0.67, respectively). The risk decrease by post-diagnostic statin use was likely affected by healthy adherer bias; that is, the greater likelihood of dying cancer patients to discontinue statin use as the association was not clearly dose-dependent and observed already at low-dose/short-term use. The dose- and time-dependence of the survival benefit among pre-diagnostic statin users suggests a possible causal effect that should be evaluated further in a clinical trial testing statins' effect on survival in breast cancer patients.",
  "metadata": {
    "url": "http://www.ncbi.nlm.nih.gov/pubmed/25329299"
  }
}
\end{lstlisting}

\textbf{Example Raw Input (queries.jsonl line 1)}:

\begin{lstlisting}[language=json]
{
  "_id": "PLAIN-3",
  "text": "Do Cholesterol Statin Drugs Cause Breast Cancer?",
  "metadata": {
    "url": "http://nutritionfacts.org/video/do-cholesterol-statin-drugs-cause-breast-cancer"
  }
}
\end{lstlisting}

\textbf{Example Raw Input (qrels/dev.tsv lines 1-3)}:

\begin{lstlisting}[language=tsv]
query-id	corpus-id	score
PLAIN-1	MED-2421	2
PLAIN-1	MED-2419	1
PLAIN-1	MED-2414	1
\end{lstlisting}

\paragraph{ \textbf{PROCESSING}}

\begin{lstlisting}[language=python]
# Step 1: Load corpus using BeIR GenericDataLoader
corpus, queries, qrels = GenericDataLoader(data_folder=dataset_path).load(split="test")

# Internal processing:
# - Parse JSONL files â†’ Python dictionaries
# - corpus: Dict[str, Dict] = {
#     'MED-10': {
#         'title': 'Statin Use and Breast Cancer Survival...',
#         'text': 'Recent studies have suggested...',
#         'metadata': {...}
#     },
#     'MED-14': {...},
#     ...
# }

# Step 2: Prepare for indexing
def prepare_corpus_for_indexing(corpus):
    documents = []
    for doc_id, doc_data in corpus.items():
        # Combine title + text for full-text indexing
        full_text = f"{doc_data['title']}\n\n{doc_data['text']}"
        
        documents.append({
            'id': doc_id,                    # Original ID (e.g., 'MED-10')
            'title': doc_data['title'],      # Document title
            'text': doc_data['text'],        # Abstract/snippet
            'full_text': full_text           # Title + text combined
        })
    return documents
\end{lstlisting}

\paragraph{ \textbf{OUTPUT}}

\begin{lstlisting}[language=python]
# Type: Tuple[Dict, Dict, Dict]
corpus = {
    'MED-10': {
        'title': 'Statin Use and Breast Cancer Survival...',
        'text': 'Recent studies have suggested that statins...',
        'metadata': {'url': 'http://www.ncbi.nlm.nih.gov/pubmed/25329299'}
    },
    'MED-14': {...},
    ... (3,633 documents total)
}

queries = {
    'PLAIN-3': 'Do Cholesterol Statin Drugs Cause Breast Cancer?',
    'PLAIN-4': 'Using Diet to Treat Asthma',
    ... (323 queries total)
}

qrels = {
    'PLAIN-1': {
        'MED-2421': 2,  # Relevance score (0-2)
        'MED-2419': 1,
        'MED-2414': 1,
        ... (38 relevant docs for this query)
    },
    'PLAIN-3': {...},
    ... (323 query-document mappings total)
}

# Prepared for indexing:
documents = [
    {
        'id': 'MED-10',
        'title': 'Statin Use and Breast Cancer Survival...',
        'text': 'Recent studies have suggested...',
        'full_text': 'Statin Use and Breast Cancer Survival...\n\nRecent studies have suggested...'
    },
    ... (3,633 documents)
]
\end{lstlisting}

\textbf{Output Statistics}:

\begin{itemize}
  \item Corpus: 3,633 medical documents
  \item Queries: 323 test queries
  \item Qrels: 12,334 relevance judgments (avg 38.19 relevant docs per query)
  \item Documents for indexing: 3,633 prepared dictionaries
\end{itemize}

\subsection{ PHASE 2: PROCESSING LAYER - Document Chunking}

\subsubsection{ Module: \texttt{llamaindex\_bm25.py} / \texttt{llamaindex\_rag.py} â†’ \texttt{SentenceSplitter}}

\paragraph{ \textbf{INPUT}}

\begin{lstlisting}[language=python]
# From Phase 1 output
documents = [
    {
        'id': 'MED-10',
        'full_text': 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland\n\nRecent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality, could delay or prevent breast cancer recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of breast cancer death among statin users in a population-based cohort of breast cancer patients. The study cohort included all newly diagnosed breast cancer patients in Finland during 1995â€“2003 (31,236 cases), identified from the Finnish Cancer Registry. Information on statin use before and after the diagnosis was obtained from a national prescription database. We used the Cox proportional hazards regression method to estimate mortality among statin users with statin use as time-dependent variable. A total of 4,151 participants had used statins. During the median follow-up of 3.25 years after the diagnosis (range 0.08â€“9.0 years) 6,011 participants died, of which 3,619 (60.2%) was due to breast cancer. After adjustment for age, tumor characteristics, and treatment selection, both post-diagnostic and pre-diagnostic statin use were associated with lowered risk of breast cancer death (HR 0.46, 95% CI 0.38â€“0.55 and HR 0.54, 95% CI 0.44â€“0.67, respectively). The risk decrease by post-diagnostic statin use was likely affected by healthy adherer bias; that is, the greater likelihood of dying cancer patients to discontinue statin use as the association was not clearly dose-dependent and observed already at low-dose/short-term use. The dose- and time-dependence of the survival benefit among pre-diagnostic statin users suggests a possible causal effect that should be evaluated further in a clinical trial testing statins' effect on survival in breast cancer patients.',
        ... (metadata)
    },
    ... (3,633 documents)
]
\end{lstlisting}

\textbf{Input Characteristics}:

\begin{itemize}
  \item Document length: 90-9,939 characters (avg \textasciitilde{}1,497 chars)
  \item Contains: Title + Abstract/Full text
  \item Format: Plain text string
\end{itemize}

\paragraph{ \textbf{PROCESSING}}

\begin{lstlisting}[language=python]
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core import Document

# Step 1: Convert to LlamaIndex Document objects
llama_docs = []
for doc in documents:
    llama_doc = Document(
        text=doc['full_text'],          # Full text content
        metadata={
            'doc_id': doc['id'],         # 'MED-10'
            'title': doc['title'],       # Document title
            'text_snippet': doc['text'][:200]  # First 200 chars for display
        },
        id_=doc['id']                   # Unique ID
    )
    llama_docs.append(llama_doc)

# Step 2: Initialize SentenceSplitter
node_parser = SentenceSplitter(
    chunk_size=512,        # Max 512 tokens per chunk
    chunk_overlap=50       # 50 tokens overlap between chunks
)

# Step 3: Split documents into nodes (chunks)
nodes = node_parser.get_nodes_from_documents(llama_docs)

# Internal processing logic:
# For each document:
#   1. Tokenize text using tiktoken (GPT tokenizer)
#   2. Split into chunks of 512 tokens
#   3. Add 50-token overlap with previous chunk
#   4. Create Node object for each chunk
#   5. Preserve metadata from parent document

# Example chunking for MED-10 (1,497 chars â‰ˆ 350 tokens):
# Chunk 1 (tokens 0-512):
#   "Statin Use and Breast Cancer Survival...[first 512 tokens]..."
# Chunk 2 would start at token 462 (512-50) if document was longer
\end{lstlisting}

\textbf{Chunking Algorithm Details}:

\begin{lstlisting}
Original Document: 1,500 tokens
â”œâ”€ Chunk 1: tokens [0:512]           (512 tokens)
â”œâ”€ Chunk 2: tokens [462:974]         (512 tokens, overlaps 50 with Chunk 1)
â””â”€ Chunk 3: tokens [924:1436]        (512 tokens, overlaps 50 with Chunk 2)
â””â”€ Chunk 4: tokens [1386:1500]       (114 tokens, remainder)

Why overlap?
- Prevents context loss at chunk boundaries
- Improves retrieval recall for queries spanning boundaries
- Standard practice in RAG systems
\end{lstlisting}

\paragraph{ \textbf{OUTPUT}}

\begin{lstlisting}[language=python]
# Type: List[Node]
nodes = [
    Node(
        id_='MED-10_chunk_0',                    # Auto-generated chunk ID
        text='Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland\n\nRecent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality, could delay or prevent breast cancer recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of breast cancer death among statin users in a population-based cohort of breast cancer patients. The study cohort included all newly diagnosed breast cancer patients in Finland during 1995â€“2003 (31,236 cases), identified from the Finnish Cancer Registry. Information on statin use before and after the diagnosis was obtained from a national prescription database. We used the Cox proportional hazards regression method to estimate mortality among statin users with statin use as time-dependent variable. A total of 4,151 participants had used statins. During the median follow-up of 3.25 years after the diagnosis (range 0.08â€“9.0 years) 6,011 participants died, of which 3,619 (60.2%) was due to breast cancer.',
        metadata={
            'doc_id': 'MED-10',                  # Parent document ID
            'title': 'Statin Use and Breast Cancer Survival...',
            'text_snippet': 'Recent studies have suggested...'
        },
        relationships={
            'SOURCE': 'MED-10'                   # Link to parent document
        }
    ),
    Node(
        id_='MED-14_chunk_0',
        text='...',
        metadata={...}
    ),
    ... (117 nodes from 100 documents in demo)
    ... (4,276 nodes from full 3,633 documents)
]
\end{lstlisting}

\textbf{Output Statistics}:

\begin{itemize}
  \item Input: 3,633 documents
  \item Output: \textasciitilde{}4,276 nodes (chunks)
  \item Avg chunks per document: 1.18 (most docs fit in 1 chunk)
  \item Node size: 512 tokens max (\textasciitilde{}2,048 chars)
\end{itemize}

\subsection{ PHASE 3A: STORAGE LAYER - BM25 Sparse Index}

\subsubsection{ Module: \texttt{llamaindex\_bm25.py} â†’ \texttt{BM25Retriever} + \texttt{SimpleDocumentStore}}

\paragraph{ \textbf{INPUT}}

\begin{lstlisting}[language=python]
# From Phase 2 output
nodes = [
    Node(id_='MED-10_chunk_0', text='...', metadata={...}),
    Node(id_='MED-14_chunk_0', text='...', metadata={...}),
    ... (4,276 nodes)
]
\end{lstlisting}

\paragraph{ \textbf{PROCESSING}}

\begin{lstlisting}[language=python]
from llama_index.core.storage.docstore import SimpleDocumentStore
from llama_index.retrievers.bm25 import BM25Retriever

# Step 1: Create docstore and store nodes
docstore = SimpleDocumentStore()
docstore.add_documents(nodes)

# Internal: Docstore stores nodes in dict
# docstore.docs = {
#     'MED-10_chunk_0': Node(...),
#     'MED-14_chunk_0': Node(...),
#     ...
# }

# Step 2: Build BM25 index from docstore
retriever = BM25Retriever.from_defaults(
    docstore=docstore,
    similarity_top_k=100
)

# Internal BM25 index building process:
# 1. Tokenize all node texts
#    - Use bm25s library tokenizer
#    - Remove stopwords ('the', 'a', 'is', etc.)
#    - Apply stemming ('running' â†’ 'run')
#
# 2. Build inverted index
#    - For each term, track which nodes contain it
#    - Store term frequencies
#
# 3. Calculate IDF (Inverse Document Frequency)
#    - IDF(term) = log((N - df + 0.5) / (df + 0.5))
#    - N = total nodes (4,276)
#    - df = nodes containing term
#
# Example for 'cancer':
#   - df = 790 nodes contain 'cancer'
#   - IDF = log((4276 - 790 + 0.5) / (790 + 0.5)) = log(4.41) â‰ˆ 1.48

# Step 3: Store BM25 index structure in memory
# self.retriever.index = {
#     'vocab': ['statin', 'breast', 'cancer', 'survival', ...],  # Unique terms
#     'doc_lens': [350, 420, 280, ...],                          # Length of each node
#     'avgdl': 310,                                              # Average doc length
#     'idf': {
#         'statin': 2.15,
#         'breast': 2.08, 
#         'cancer': 1.48,
#         ...
#     },
#     'inverted_index': {
#         'statin': {
#             'MED-10_chunk_0': [1, 45],      # Positions where 'statin' appears
#             'MED-14_chunk_0': [5],
#             ...
#         },
#         'cancer': {
#             'MED-10_chunk_0': [8, 15, 72],
#             ...
#         }
#     }
# }

# Step 4: Persist to disk (NEW after migration)
docstore.persist("./storage/bm25/docstore.json")

# docstore.json structure:
# {
#   "MED-10_chunk_0": {
#     "text": "Statin Use and Breast Cancer...",
#     "metadata": {"doc_id": "MED-10", "title": "..."},
#     "id_": "MED-10_chunk_0"
#   },
#   "MED-14_chunk_0": {...},
#   ...
# }
\end{lstlisting}

\textbf{BM25 Index Structure Visual}:

\begin{lstlisting}
Inverted Index (Word â†’ Documents):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

'cancer' â†’ {
    'MED-10_chunk_0': {freq: 3, positions: [8, 15, 72], tf: 0.0086},
    'MED-14_chunk_0': {freq: 1, positions: [12], tf: 0.0024},
    'MED-118_chunk_0': {freq: 2, positions: [5, 34], tf: 0.0071},
    ... (790 nodes total)
}

'statin' â†’ {
    'MED-10_chunk_0': {freq: 2, positions: [1, 45], tf: 0.0057},
    'MED-14_chunk_0': {freq: 1, positions: [5], tf: 0.0024},
    ... (50 nodes total)
}

IDF Values:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
'cancer': 1.48 (common term, appears in 790/4276 = 18.5% of nodes)
'statin': 2.15 (rare term, appears in 50/4276 = 1.2% of nodes)
'the': -4.68 (very common, appears in ~4000/4276 = 93% of nodes)
\end{lstlisting}

\paragraph{ \textbf{OUTPUT}}

\begin{lstlisting}[language=python]
# In-memory BM25 index (retriever object)
retriever = BM25Retriever(
    docstore=docstore,                    # Reference to persistent docstore
    similarity_top_k=100,                 # Default top-k
    # Internal state:
    # - vocab: 15,423 unique terms
    # - inverted_index: term â†’ node mappings
    # - idf_scores: pre-computed IDF values
    # - avgdl: 310 tokens
)

# Persistent storage on disk
"./storage/bm25/docstore.json" = {
    "MED-10_chunk_0": {...},
    "MED-14_chunk_0": {...},
    ... (4,276 node objects, ~2.5 MB JSON file)
}
\end{lstlisting}

\textbf{Output Characteristics}:

\begin{itemize}
  \item Inverted index: \textasciitilde{}15,423 unique terms
  \item Index size in RAM: \textasciitilde{}50 MB (sparse representation)
  \item Docstore on disk: \textasciitilde{}2.5 MB (JSON format)
  \item Load time: \textasciitilde{}2 seconds (rebuild BM25 from docstore)
\end{itemize}

\subsection{ PHASE 3B: STORAGE LAYER - Dense Vector Index}

\subsubsection{ Module: \texttt{llamaindex\_rag.py} â†’ \texttt{PGVectorStore} + \texttt{VectorStoreIndex}}

\paragraph{ \textbf{INPUT}}

\begin{lstlisting}[language=python]
# From Phase 2 output (LlamaIndex Document objects, not chunked yet)
llama_docs = [
    Document(
        text='Statin Use and Breast Cancer Survival...\n\nRecent studies...',
        metadata={'doc_id': 'MED-10', 'title': '...'},
        id_='MED-10'
    ),
    ... (3,633 documents)
]

# PostgreSQL connection params from .env
POSTGRES_HOST = 'localhost'
POSTGRES_PORT = 5433
POSTGRES_DB = 'beir_benchmark'
POSTGRES_USER = 'beir_user'
POSTGRES_PASSWORD = 'beir_password'
\end{lstlisting}

\paragraph{ \textbf{PROCESSING}}

\begin{lstlisting}[language=python]
from llama_index.core import VectorStoreIndex, StorageContext, Settings
from llama_index.vector_stores.postgres import PGVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# Step 1: Configure embedding model
embed_model = HuggingFaceEmbedding(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
Settings.embed_model = embed_model
Settings.chunk_size = 512
Settings.chunk_overlap = 50

# Step 2: Create PostgreSQL vector store
vector_store = PGVectorStore.from_params(
    database="beir_benchmark",
    host="localhost",
    port=5433,
    user="beir_user",
    password="beir_password",
    table_name="nfcorpus_embeddings",
    embed_dim=384  # all-MiniLM-L6-v2 produces 384-dim vectors
)

# Internal: Creates PostgreSQL table
# CREATE TABLE nfcorpus_embeddings (
#     id TEXT PRIMARY KEY,
#     text TEXT,
#     metadata JSONB,
#     embedding VECTOR(384)  -- pgvector extension
# );

# Step 3: Create storage context
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# Step 4: Build index (auto-embed + store)
index = VectorStoreIndex.from_documents(
    llama_docs,
    storage_context=storage_context,
    show_progress=True
)

# Internal processing (for each document):
# 1. Chunk document using SentenceSplitter (512/50)
#    â†’ Same as BM25, produces ~4,276 chunks
#
# 2. Generate embeddings for each chunk
#    Example for 'MED-10_chunk_0':
#    
#    Input text: "Statin Use and Breast Cancer Survival..."
#    â†“
#    Tokenize: ['statin', 'use', 'and', 'breast', 'cancer', ...]
#    â†“
#    Pass through neural network (all-MiniLM-L6-v2):
#    - 6-layer transformer model
#    - Mean pooling of token embeddings
#    â†“
#    Output: 384-dimensional vector
#    [0.234, -0.156, 0.891, ..., 0.045]  (384 floats)
#
# 3. Store in PostgreSQL
#    INSERT INTO nfcorpus_embeddings (id, text, metadata, embedding)
#    VALUES (
#        'MED-10_chunk_0',
#        'Statin Use and Breast Cancer Survival...',
#        '{"doc_id": "MED-10", "title": "..."}',
#        '[0.234, -0.156, 0.891, ..., 0.045]'
#    );
#
# 4. Build HNSW index for fast vector search
#    CREATE INDEX ON nfcorpus_embeddings 
#    USING hnsw (embedding vector_cosine_ops);

# Embedding generation time:
# - all-MiniLM-L6-v2 speed: ~500 sequences/sec on CPU
# - 4,276 chunks / 500 = ~8.5 seconds
# - With batch size 32: ~5 seconds total
\end{lstlisting}

\textbf{Vector Embedding Example}:

\begin{lstlisting}
Text: "Statin Use and Breast Cancer Survival"

After embedding (384 dimensions):
[
    0.234,   # Dimension 0: captures "medical" semantic
    -0.156,  # Dimension 1: captures "drug" semantic
    0.891,   # Dimension 2: captures "breast cancer" semantic
    0.421,
    -0.678,
    ... (384 dimensions total)
    0.045
]

Properties:
- Cosine similarity with similar texts: > 0.7
- Cosine similarity with unrelated texts: < 0.3
- L2 norm: ~1.0 (normalized vector)
\end{lstlisting}

\textbf{PostgreSQL Storage Structure}:

\begin{lstlisting}[language=sql]
-- Table: nfcorpus_embeddings
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id               â”‚ text                        â”‚ metadata             â”‚ embedding                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MED-10_chunk_0   â”‚ Statin Use and Breast...    â”‚ {"doc_id":"MED-10"}  â”‚ [0.234,-0.156,0.891,...]  â”‚
â”‚ MED-14_chunk_0   â”‚ Statin use after...         â”‚ {"doc_id":"MED-14"}  â”‚ [0.198,-0.234,0.823,...]  â”‚
â”‚ MED-118_chunk_0  â”‚ Alkylphenols and cancer...  â”‚ {"doc_id":"MED-118"} â”‚ [0.145,-0.089,0.912,...]  â”‚
â”‚ ...              â”‚ ...                         â”‚ ...                  â”‚ ...                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

-- HNSW Index for fast approximate nearest neighbor search
-- Index name: nfcorpus_embeddings_embedding_idx
-- Type: HNSW (Hierarchical Navigable Small World)
-- Distance metric: Cosine similarity
-- Construction time: ~10 seconds for 4,276 vectors
\end{lstlisting}

\paragraph{ \textbf{OUTPUT}}

\begin{lstlisting}[language=python]
# VectorStoreIndex object (connected to PostgreSQL)
index = VectorStoreIndex(
    storage_context=storage_context,
    # Internal state:
    # - Connected to PostgreSQL table 'nfcorpus_embeddings'
    # - 4,276 vectors stored
    # - HNSW index built for fast search
)

# PostgreSQL database state:
# Table: nfcorpus_embeddings
# - 4,276 rows (one per chunk)
# - ~6.5 MB (4276 * 384 floats * 4 bytes = 6.5 MB vectors + text/metadata)
# - HNSW index: ~2 MB additional storage

# Persistent storage (automatic):
# - All data in PostgreSQL on disk
# - Survives process restart
# - Can be queried from multiple clients
\end{lstlisting}

\textbf{Output Characteristics}:

\begin{itemize}
  \item Total embeddings: 4,276 vectors
  \item Vector dimension: 384 (all-MiniLM-L6-v2)
  \item Storage size: \textasciitilde{}8.5 MB (vectors + metadata + index)
  \item Query speed: \textasciitilde{}50ms for top-100 search (with HNSW)
\end{itemize}

\subsection{ PHASE 4A: RETRIEVAL LAYER - BM25 Search}

\subsubsection{ Module: \texttt{llamaindex\_bm25.py} â†’ \texttt{search()}}

\paragraph{ \textbf{INPUT}}

\begin{lstlisting}[language=python]
# User query
query = "Do Cholesterol Statin Drugs Cause Breast Cancer?"
top_k = 10  # Number of results to return
\end{lstlisting}

\paragraph{ \textbf{PROCESSING}}

\begin{lstlisting}[language=python]
def search(self, query: str, top_k: int = 10) -> List[Dict]:
    # Step 1: Tokenize query
    # Input: "Do Cholesterol Statin Drugs Cause Breast Cancer?"
    # After stopword removal and stemming:
    query_terms = ['cholesterol', 'statin', 'drug', 'caus', 'breast', 'cancer']
    
    # Step 2: Look up terms in inverted index
    # For each term, find candidate documents
    candidates = {}
    
    # Term 1: 'cholesterol'
    #   - IDF = 2.45 (appears in 30/4276 nodes)
    #   - Found in: [MED-10_chunk_0, MED-666_chunk_0, ...]
    
    # Term 2: 'statin'
    #   - IDF = 2.15 (appears in 50/4276 nodes)
    #   - Found in: [MED-10_chunk_0, MED-14_chunk_0, ...]
    
    # Term 3: 'breast'
    #   - IDF = 2.08 (appears in 150/4276 nodes)
    #   - Found in: [MED-10_chunk_0, MED-666_chunk_0, ...]
    
    # Term 4: 'cancer'
    #   - IDF = 1.48 (appears in 790/4276 nodes)
    #   - Found in: [MED-10_chunk_0, MED-14_chunk_0, ...]
    
    # Union of candidates: ~850 nodes contain at least one query term
    
    # Step 3: Calculate BM25 score for each candidate
    # For MED-10_chunk_0:
    doc_id = 'MED-10_chunk_0'
    doc_length = 350  # tokens
    avgdl = 310
    k1 = 1.5  # BM25 parameter
    b = 0.75  # BM25 parameter
    
    bm25_score = 0.0
    
    # For each query term:
    for term in query_terms:
        if term in document:
            freq = term_frequency_in_doc[term]  # How many times term appears
            idf = self.idf[term]                 # Pre-computed IDF
            
            # BM25 formula component
            numerator = freq * (k1 + 1)
            denominator = freq + k1 * (1 - b + b * (doc_length / avgdl))
            
            bm25_score += idf * (numerator / denominator)
    
    # Example calculation for MED-10_chunk_0:
    # 'cholesterol': freq=1, IDF=2.45
    #   â†’ contribution = 2.45 * (1*2.5)/(1+1.5*(1-0.75+0.75*350/310))
    #   â†’ contribution = 2.45 * 2.5/2.59 = 2.37
    
    # 'statin': freq=2, IDF=2.15
    #   â†’ contribution = 2.15 * (2*2.5)/(2+1.5*1.13)
    #   â†’ contribution = 2.15 * 5.0/3.69 = 2.91
    
    # 'breast': freq=3, IDF=2.08
    #   â†’ contribution = 2.08 * (3*2.5)/(3+1.5*1.13) = 3.43
    
    # 'cancer': freq=4, IDF=1.48
    #   â†’ contribution = 1.48 * (4*2.5)/(4+1.5*1.13) = 2.87
    
    # Total BM25 score = 2.37 + 2.91 + 3.43 + 2.87 = 11.58 â­
    
    # Step 4: Sort candidates by score
    ranked_results = sorted(candidates.items(), key=lambda x: x[1], reverse=True)
    
    # Step 5: Get top-k results
    top_results = ranked_results[:top_k]
    
    # Step 6: Format output
    results = []
    for rank, (node_id, score) in enumerate(top_results, 1):
        doc_id = node_metadata['doc_id']  # e.g., 'MED-10'
        title = node_metadata['title']
        text = node_metadata['text_snippet']
        
        results.append({
            'id': doc_id,
            'score': score,
            'title': title,
            'text': text,
            'rank': rank
        })
    
    return results
\end{lstlisting}

\textbf{BM25 Scoring Visual Example}:

\begin{lstlisting}
Query: "cholesterol statin breast cancer"
Query Terms After Processing: ['cholesterol', 'statin', 'breast', 'cancer']

Document: MED-10_chunk_0 (350 tokens)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Term Analysis:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Term        â”‚ Freq â”‚ IDF  â”‚ TF Component    â”‚ Contribution â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ cholesterol â”‚  1   â”‚ 2.45 â”‚ 1.0/(1+1.5*...) â”‚ 2.37         â”‚
â”‚ statin      â”‚  2   â”‚ 2.15 â”‚ 1.73/(2+1.5*..) â”‚ 2.91         â”‚
â”‚ breast      â”‚  3   â”‚ 2.08 â”‚ 1.85/(3+1.5*..) â”‚ 3.43         â”‚
â”‚ cancer      â”‚  4   â”‚ 1.48 â”‚ 2.22/(4+1.5*..) â”‚ 2.87         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total BM25 Score: 11.58 ğŸ¯
Rank: #1

Why this score is high:
âœ… Contains ALL query terms (4/4 match)
âœ… High term frequencies (statin:2, breast:3, cancer:4)
âœ… Rare terms present (cholesterol, statin have high IDF)
\end{lstlisting}

\paragraph{ \textbf{OUTPUT}}

\begin{lstlisting}[language=python]
# Type: List[Dict]
results = [
    {
        'id': 'MED-10',
        'score': 11.58,
        'title': 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland',
        'text': 'Recent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality, could delay or prevent breast cancer recurrence...',
        'rank': 1
    },
    {
        'id': 'MED-14',
        'score': 9.23,
        'title': 'Statin use after diagnosis of breast cancer and survival',
        'text': 'Statin use after breast cancer diagnosis has been associated with improved outcomes...',
        'rank': 2
    },
    {
        'id': 'MED-666',
        'score': 7.89,
        'title': 'Cholesterol-lowering drugs and cancer risk',
        'text': 'Cholesterol-lowering medications, including statins, have been studied for potential effects on cancer...',
        'rank': 3
    },
    ... (10 results total)
]
\end{lstlisting}

\textbf{Output Characteristics}:

\begin{itemize}
  \item Top-10 documents ranked by BM25 score
  \item Scores range: 11.58 to 3.45 (decreasing)
  \item All results contain at least one query term
  \item Lexical matching only (no semantic understanding)
\end{itemize}

\subsection{ PHASE 4B: RETRIEVAL LAYER - Dense Search}

\subsubsection{ Module: \texttt{llamaindex\_rag.py} â†’ \texttt{search()}}

\paragraph{ \textbf{INPUT}}

\begin{lstlisting}[language=python]
# User query
query = "Do Cholesterol Statin Drugs Cause Breast Cancer?"
top_k = 10
\end{lstlisting}

\paragraph{ \textbf{PROCESSING}}

\begin{lstlisting}[language=python]
def search(self, query: str, top_k: int = 10) -> List[Dict]:
    # Step 1: Generate query embedding
    # Input: "Do Cholesterol Statin Drugs Cause Breast Cancer?"
    # 
    # Process:
    # 1. Tokenize: ['do', 'cholesterol', 'statin', 'drugs', 'cause', 'breast', 'cancer']
    # 2. Pass through all-MiniLM-L6-v2 transformer
    # 3. Mean pooling of token embeddings
    # 
    # Output: 384-dimensional vector
    query_embedding = embed_model.get_query_embedding(query)
    # query_embedding = [0.198, -0.145, 0.867, ..., 0.034]  (384 floats)
    
    # Step 2: Search PostgreSQL with vector similarity
    # SQL query (internal):
    # SELECT id, text, metadata, embedding,
    #        1 - (embedding <=> query_embedding) AS similarity
    # FROM nfcorpus_embeddings
    # ORDER BY embedding <=> query_embedding  -- Cosine distance operator
    # LIMIT 100;
    
    # Vector similarity calculation:
    # For each document embedding in database:
    # cosine_similarity = (query_vec Â· doc_vec) / (||query_vec|| Ã— ||doc_vec||)
    
    # Example for MED-10_chunk_0:
    doc_embedding = [0.234, -0.156, 0.891, ..., 0.045]
    
    # Dot product
    dot_product = sum(q * d for q, d in zip(query_embedding, doc_embedding))
    # dot_product = (0.198*0.234) + (-0.145*-0.156) + (0.867*0.891) + ... â‰ˆ 0.823
    
    # Norms (assuming pre-normalized vectors)
    # ||query_vec|| = 1.0
    # ||doc_vec|| = 1.0
    
    # Cosine similarity
    similarity = dot_product / (1.0 * 1.0) = 0.823
    
    # Why high similarity?
    # - Both texts about statins and breast cancer (semantic match)
    # - Embeddings learned to cluster similar medical topics
    # - "cholesterol" and "statin" are semantically related in embedding space
    
    # Step 3: HNSW index accelerates search
    # Instead of checking all 4,276 vectors:
    # - Navigate HNSW graph structure
    # - Only compute ~50-100 dot products (instead of 4,276)
    # - Find approximate k-nearest neighbors
    # - Search time: O(log N) instead of O(N)
    
    # Step 4: Rank by similarity score
    # PostgreSQL returns top-100 most similar vectors
    retrieved_nodes = [
        ('MED-10_chunk_0', 0.823),   # Highest similarity
        ('MED-14_chunk_0', 0.801),
        ('MED-666_chunk_0', 0.789),
        ('MED-118_chunk_0', 0.756),  # Still relevant (alkylphenols & cancer)
        ... (100 results total)
    ]
    
    # Step 5: Get top-k and format
    results = []
    for rank, (node_id, score) in enumerate(retrieved_nodes[:top_k], 1):
        # Retrieve metadata from PostgreSQL
        doc_id = node_metadata['doc_id']
        title = node_metadata['title']
        text = node_metadata['text_snippet']
        
        results.append({
            'id': doc_id,
            'score': score,
            'title': title,
            'text': text,
            'rank': rank
        })
    
    return results
\end{lstlisting}

\textbf{Vector Similarity Visual}:

\begin{lstlisting}
Query Embedding (384-dim):
[0.198, -0.145, 0.867, 0.421, -0.234, 0.567, ...]

Document Embeddings (Top 3):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MED-10 (Statin & Breast Cancer):
[0.234, -0.156, 0.891, 0.398, -0.198, 0.543, ...]
Cosine Similarity: 0.823 â­â­â­ (Very similar)
â†’ Semantically very close (same topic)

MED-14 (Statin after diagnosis):
[0.212, -0.134, 0.856, 0.411, -0.223, 0.521, ...]
Cosine Similarity: 0.801 â­â­â­ (Very similar)
â†’ Related topic, slightly different angle

MED-666 (Cholesterol & Cancer Risk):
[0.189, -0.167, 0.834, 0.378, -0.256, 0.498, ...]
Cosine Similarity: 0.789 â­â­ (Similar)
â†’ Broader topic overlap

MED-3000 (Vitamin D deficiency):
[0.045, -0.023, 0.123, 0.089, -0.034, 0.012, ...]
Cosine Similarity: 0.234 âŒ (Not similar)
â†’ Different topic entirely

Similarity Threshold for Relevance: > 0.7
\end{lstlisting}

\textbf{Semantic Understanding Advantage}:

\begin{lstlisting}
Query: "cholesterol statin breast cancer"

BM25 (Lexical):
âŒ Misses: "lipid-lowering drugs" (synonym for statins)
âŒ Misses: "mammary carcinoma" (synonym for breast cancer)
âœ… Finds: Exact word matches only

Dense (Semantic):
âœ… Finds: "lipid-lowering drugs" â†’ embedding close to "statin"
âœ… Finds: "mammary carcinoma" â†’ embedding close to "breast cancer"
âœ… Finds: "atorvastatin" â†’ specific statin drug
âœ… Understands context and relationships
\end{lstlisting}

\paragraph{ \textbf{OUTPUT}}

\begin{lstlisting}[language=python]
# Type: List[Dict]
results = [
    {
        'id': 'MED-10',
        'score': 0.823,  # Cosine similarity (0-1)
        'title': 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland',
        'text': 'Recent studies have suggested that statins, an established drug group...',
        'rank': 1
    },
    {
        'id': 'MED-14',
        'score': 0.801,
        'title': 'Statin use after diagnosis of breast cancer and survival',
        'text': 'Statin use after breast cancer diagnosis has been associated...',
        'rank': 2
    },
    {
        'id': 'MED-666',
        'score': 0.789,
        'title': 'Cholesterol-lowering drugs and cancer risk',
        'text': 'Cholesterol-lowering medications, including statins...',
        'rank': 3
    },
    {
        'id': 'MED-118',  # Note: Different from BM25 results!
        'score': 0.756,
        'title': 'Alkylphenols and estrogen-related breast cancer',
        'text': 'Alkylphenols have been studied for potential cancer effects...',
        'rank': 4
    },
    ... (10 results total)
]
\end{lstlisting}

\textbf{Output Characteristics}:

\begin{itemize}
  \item Top-10 documents ranked by cosine similarity
  \item Scores range: 0.823 to 0.701 (all above 0.7 threshold)
  \item May include synonym matches not in BM25
  \item Semantic understanding captures intent
\end{itemize}

\subsection{ PHASE 4C: RETRIEVAL LAYER - Hybrid Search (RRF)}

\subsubsection{ Module: \texttt{llamaindex\_hybrid.py} â†’ \texttt{search()}}

\paragraph{ \textbf{INPUT}}

\begin{lstlisting}[language=python]
# User query
query = "Do Cholesterol Statin Drugs Cause Breast Cancer?"
top_k = 10

# BM25 and Dense retrievers (already initialized)
bm25_retriever = LlamaIndexBM25(...)
dense_retriever = LlamaIndexRAG(...)

# RRF parameters
alpha = 0.5  # Equal weight to BM25 and Dense
k = 60       # RRF constant
\end{lstlisting}

\paragraph{ \textbf{PROCESSING}}

\begin{lstlisting}[language=python]
def search(self, query: str, top_k: int = 10) -> List[Dict]:
    # Step 1: Retrieve from both systems
    # Get top-100 from each to have enough candidates for fusion
    
    bm25_results = self.bm25.search(query, top_k=100)
    # [
    #   {'id': 'MED-10', 'score': 11.58, 'rank': 1},
    #   {'id': 'MED-14', 'score': 9.23, 'rank': 2},
    #   {'id': 'MED-666', 'score': 7.89, 'rank': 3},
    #   ... (100 results)
    # ]
    
    dense_results = self.dense.search(query, top_k=100)
    # [
    #   {'id': 'MED-10', 'score': 0.823, 'rank': 1},
    #   {'id': 'MED-14', 'score': 0.801, 'rank': 2},
    #   {'id': 'MED-666', 'score': 0.789, 'rank': 3},
    #   {'id': 'MED-118', 'score': 0.756, 'rank': 4},
    #   ... (100 results)
    # ]
    
    # Step 2: Apply RRF (Reciprocal Rank Fusion)
    fused_scores = {}
    doc_info = {}
    doc_sources = {}  # Track which system found each doc
    
    # Process BM25 results
    for result in bm25_results:
        doc_id = result['id']
        rank = result['rank']  # 1, 2, 3, ...
        
        # RRF score contribution from BM25
        # score_bm25(d) = Î± / (k + rank_bm25(d))
        bm25_contribution = self.alpha / (self.k + rank)
        
        fused_scores[doc_id] = bm25_contribution
        doc_info[doc_id] = {'title': result['title'], 'text': result['text']}
        doc_sources[doc_id] = 'bm25'
    
    # Example for MED-10 (rank=1 in BM25):
    # bm25_contribution = 0.5 / (60 + 1) = 0.5 / 61 â‰ˆ 0.0082
    
    # Example for MED-666 (rank=3 in BM25):
    # bm25_contribution = 0.5 / (60 + 3) = 0.5 / 63 â‰ˆ 0.0079
    
    # Process Dense results
    for result in dense_results:
        doc_id = result['id']
        rank = result['rank']
        
        # RRF score contribution from Dense
        # score_dense(d) = (1-Î±) / (k + rank_dense(d))
        dense_contribution = (1 - self.alpha) / (self.k + rank)
        
        if doc_id in fused_scores:
            # Document found in BOTH systems â†’ add contributions
            fused_scores[doc_id] += dense_contribution
            doc_sources[doc_id] = 'both'  # â­ Best case
        else:
            # Document only in Dense
            fused_scores[doc_id] = dense_contribution
            doc_info[doc_id] = {'title': result['title'], 'text': result['text']}
            doc_sources[doc_id] = 'dense'
    
    # Example for MED-10 (rank=1 in both):
    # bm25_contribution = 0.5 / 61 â‰ˆ 0.0082
    # dense_contribution = 0.5 / 61 â‰ˆ 0.0082
    # Total fused_score = 0.0164 â­â­ (highest possible)
    
    # Example for MED-666 (rank=3 in BM25, rank=3 in Dense):
    # bm25_contribution = 0.5 / 63 â‰ˆ 0.0079
    # dense_contribution = 0.5 / 63 â‰ˆ 0.0079
    # Total fused_score = 0.0158 â­â­
    
    # Example for MED-118 (not in BM25, rank=4 in Dense):
    # bm25_contribution = 0 (not found)
    # dense_contribution = 0.5 / 64 â‰ˆ 0.0078
    # Total fused_score = 0.0078 â­ (lower, only from one system)
    
    # Step 3: Sort by fused scores
    sorted_docs = sorted(
        fused_scores.items(),
        key=lambda x: x[1],
        reverse=True
    )
    
    # Step 4: Format top-k results
    results = []
    for rank, (doc_id, score) in enumerate(sorted_docs[:top_k], 1):
        results.append({
            'id': doc_id,
            'score': score,
            'title': doc_info[doc_id]['title'],
            'text': doc_info[doc_id]['text'],
            'rank': rank,
            'source': doc_sources[doc_id]  # 'bm25', 'dense', or 'both'
        })
    
    return results
\end{lstlisting}

\textbf{RRF Fusion Visual Example}:

\begin{lstlisting}
Query: "cholesterol statin breast cancer"

Input Rankings:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BM25 Ranking:               Dense Ranking:
1. MED-10  (score 11.58)    1. MED-10  (score 0.823)
2. MED-14  (score 9.23)     2. MED-14  (score 0.801)
3. MED-666 (score 7.89)     3. MED-666 (score 0.789)
4. MED-100 (score 6.45)     4. MED-118 (score 0.756)
5. MED-55  (score 5.12)     5. MED-100 (score 0.734)

RRF Score Calculation (Î±=0.5, k=60):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Document   â”‚ BM25 Rank â”‚ Dense Rank â”‚ BM25 Score  â”‚ Dense Score â”‚ Fused Score â”‚ Source
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€
MED-10     â”‚     1     â”‚      1     â”‚ 0.5/61=.0082â”‚ 0.5/61=.0082â”‚   0.0164    â”‚ both â­
MED-14     â”‚     2     â”‚      2     â”‚ 0.5/62=.0081â”‚ 0.5/62=.0081â”‚   0.0161    â”‚ both â­
MED-666    â”‚     3     â”‚      3     â”‚ 0.5/63=.0079â”‚ 0.5/63=.0079â”‚   0.0159    â”‚ both â­
MED-100    â”‚     4     â”‚      5     â”‚ 0.5/64=.0078â”‚ 0.5/65=.0077â”‚   0.0155    â”‚ both
MED-118    â”‚    -      â”‚      4     â”‚      0      â”‚ 0.5/64=.0078â”‚   0.0078    â”‚ dense
MED-55     â”‚     5     â”‚      -     â”‚ 0.5/65=.0077â”‚      0      â”‚   0.0077    â”‚ bm25

Final Hybrid Ranking:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. MED-10  (0.0164) â† both systems rank #1 â­â­
2. MED-14  (0.0161) â† both systems rank #2 â­â­
3. MED-666 (0.0159) â† both systems rank #3 â­â­
4. MED-100 (0.0155) â† both systems found it
5. MED-118 (0.0078) â† only Dense found (semantic match)
6. MED-55  (0.0077) â† only BM25 found (lexical match)

Key Insight:
âœ… Documents found by BOTH systems get highest scores
âœ… RRF normalizes different score ranges (11.58 vs 0.823)
âœ… Balances lexical precision (BM25) with semantic recall (Dense)
\end{lstlisting}

\textbf{Why RRF Works}:

\begin{lstlisting}
Problem with Score Fusion:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BM25 scores: 0-20 range
Dense scores: 0-1 range
â†’ Direct averaging would favor BM25!

RRF Solution:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Convert ranks to normalized scores:
- Rank 1 â†’ 0.5/61 = 0.0082
- Rank 2 â†’ 0.5/62 = 0.0081
- Rank 3 â†’ 0.5/63 = 0.0079
â†’ Scores now comparable!

Benefits:
âœ… Score-agnostic (only uses ranks)
âœ… Robust to score scale differences
âœ… Proven algorithm from IR research
\end{lstlisting}

\paragraph{ \textbf{OUTPUT}}

\begin{lstlisting}[language=python]
# Type: List[Dict]
results = [
    {
        'id': 'MED-10',
        'score': 0.0164,  # RRF fused score
        'title': 'Statin Use and Breast Cancer Survival...',
        'text': 'Recent studies have suggested...',
        'rank': 1,
        'source': 'both'  # â­ Found by both BM25 and Dense
    },
    {
        'id': 'MED-14',
        'score': 0.0161,
        'title': 'Statin use after diagnosis...',
        'text': 'Statin use after breast cancer...',
        'rank': 2,
        'source': 'both'
    },
    {
        'id': 'MED-666',
        'score': 0.0159,
        'title': 'Cholesterol-lowering drugs...',
        'text': 'Cholesterol-lowering medications...',
        'rank': 3,
        'source': 'both'
    },
    {
        'id': 'MED-100',
        'score': 0.0155,
        'title': 'Statins and cancer prevention',
        'text': 'The role of statins in cancer...',
        'rank': 4,
        'source': 'both'
    },
    {
        'id': 'MED-118',
        'score': 0.0078,
        'title': 'Alkylphenols and estrogen-related breast cancer',
        'text': 'Alkylphenols have been studied...',
        'rank': 5,
        'source': 'dense'  # Only found by Dense (semantic match)
    },
    ... (10 results total)
]
\end{lstlisting}

\textbf{Output Analysis}:

\begin{lstlisting}
Source Distribution (Top 10):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
both:  7 documents (70%) â­ High consensus
dense: 2 documents (20%)  â†’ Semantic-only matches
bm25:  1 document  (10%)  â†’ Lexical-only matches

Benefits:
âœ… High precision: Documents from 'both' are most reliable
âœ… High recall: Captures semantic matches BM25 misses
âœ… Balanced: Leverages strengths of both systems
\end{lstlisting}

\subsection{ PHASE 5: EVALUATION LAYER}

\subsubsection{ Module: \texttt{metrics.py} â†’ \texttt{RetrievalEvaluator}}

\paragraph{ \textbf{INPUT}}

\begin{lstlisting}[language=python]
# Retrieved results from any system (BM25, Dense, or Hybrid)
retrieved_results = {
    'PLAIN-3': [  # Query ID
        {'id': 'MED-10', 'score': 0.0164, 'rank': 1},
        {'id': 'MED-14', 'score': 0.0161, 'rank': 2},
        {'id': 'MED-666', 'score': 0.0159, 'rank': 3},
        ... (100 results)
    ],
    'PLAIN-4': [...],
    ... (323 queries total)
}

# Ground truth relevance judgments (qrels)
qrels = {
    'PLAIN-3': {
        'MED-10': 2,   # Highly relevant
        'MED-14': 2,
        'MED-2421': 1, # Relevant
        'MED-666': 1,
        ... (38 relevant docs for this query)
    },
    'PLAIN-4': {...},
    ... (323 queries)
}
\end{lstlisting}

\paragraph{ \textbf{PROCESSING}}

\begin{lstlisting}[language=python]
class RetrievalEvaluator:
    def evaluate_batch(self, results_dict: Dict, qrels: Dict) -> pd.DataFrame:
        metrics_list = []
        
        for query_id, results in results_dict.items():
            # Step 1: Extract retrieved document IDs (in rank order)
            retrieved_ids = [r['id'] for r in results]
            # ['MED-10', 'MED-14', 'MED-666', ..., 'MED-999']
            
            # Step 2: Get relevant document IDs from qrels
            relevant_ids = list(qrels.get(query_id, {}).keys())
            # ['MED-10', 'MED-14', 'MED-2421', 'MED-666', ...]
            # (38 relevant docs for PLAIN-3)
            
            # Step 3: Calculate NDCG@10
            ndcg10 = self.calculate_ndcg_at_k(retrieved_ids, relevant_ids, k=10)
            
            # NDCG@10 Calculation for PLAIN-3:
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Retrieved top-10: ['MED-10', 'MED-14', 'MED-666', 'MED-100', 'MED-118', ...]
            # Relevant docs:    ['MED-10', 'MED-14', 'MED-2421', 'MED-666', ...]
            #
            # Check each position:
            # Rank 1: MED-10  â†’ relevant âœ… â†’ gain = 1 / log2(2) = 1.0
            # Rank 2: MED-14  â†’ relevant âœ… â†’ gain = 1 / log2(3) = 0.631
            # Rank 3: MED-666 â†’ relevant âœ… â†’ gain = 1 / log2(4) = 0.5
            # Rank 4: MED-100 â†’ not relevant âŒ â†’ gain = 0
            # Rank 5: MED-118 â†’ not relevant âŒ â†’ gain = 0
            # Rank 6: MED-55  â†’ relevant âœ… â†’ gain = 1 / log2(7) = 0.356
            # ... (ranks 7-10)
            #
            # DCG@10 = 1.0 + 0.631 + 0.5 + 0 + 0 + 0.356 + ... = 2.89
            #
            # Ideal DCG (if all relevant docs at top):
            # IDCG@10 = 1.0 + 0.631 + 0.5 + 0.431 + 0.387 + 0.356 + ... = 4.12
            #
            # NDCG@10 = DCG / IDCG = 2.89 / 4.12 = 0.701 â­
            
            # Step 4: Calculate Recall@100
            recall100 = self.calculate_recall_at_k(retrieved_ids, relevant_ids, k=100)
            
            # Recall@100 Calculation for PLAIN-3:
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Retrieved top-100: [100 document IDs]
            # Relevant docs (total): 38
            #
            # How many relevant docs in top-100?
            # retrieved_set = {'MED-10', 'MED-14', 'MED-666', ..., 'MED-999'}
            # relevant_set = {'MED-10', 'MED-14', 'MED-2421', 'MED-666', ...}
            # intersection = {'MED-10', 'MED-14', 'MED-666', 'MED-55', ...}
            #
            # Found: 32 out of 38 relevant docs
            # Recall@100 = 32 / 38 = 0.842 â­
            
            # Step 5: Calculate MAP (Mean Average Precision)
            map_score = self.calculate_map(retrieved_ids, relevant_ids)
            
            # MAP Calculation for PLAIN-3:
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # For each relevant doc found, calculate precision at that rank:
            #
            # Rank 1: MED-10 (relevant) â†’ Precision = 1/1 = 1.0
            # Rank 2: MED-14 (relevant) â†’ Precision = 2/2 = 1.0
            # Rank 3: MED-666 (relevant) â†’ Precision = 3/3 = 1.0
            # Rank 6: MED-55 (relevant) â†’ Precision = 4/6 = 0.667
            # Rank 12: MED-2421 (relevant) â†’ Precision = 5/12 = 0.417
            # ... (continue for all 32 found relevant docs)
            #
            # Average Precision = (1.0 + 1.0 + 1.0 + 0.667 + 0.417 + ...) / 38
            #                   = 18.5 / 38 = 0.487 â­
            
            # Step 6: Calculate MRR (Mean Reciprocal Rank)
            mrr = self.calculate_mrr(retrieved_ids, relevant_ids)
            
            # MRR Calculation for PLAIN-3:
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Find rank of FIRST relevant document:
            # Rank 1: MED-10 â†’ relevant âœ…
            #
            # Reciprocal Rank = 1 / 1 = 1.0 â­
            #
            # (If first relevant was at rank 5: RR = 1/5 = 0.2)
            
            metrics_list.append({
                'query_id': query_id,
                'ndcg@10': ndcg10,
                'recall@100': recall100,
                'map': map_score,
                'mrr': mrr
            })
        
        # Step 7: Aggregate across all queries
        df = pd.DataFrame(metrics_list)
        
        # Calculate mean metrics
        summary = {
            'NDCG@10': df['ndcg@10'].mean(),
            'Recall@100': df['recall@100'].mean(),
            'MAP': df['map'].mean(),
            'MRR': df['mrr'].mean()
        }
        
        return df, summary
\end{lstlisting}

\textbf{Metrics Interpretation}:

\begin{lstlisting}
NDCG@10 = 0.701
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Meaning: Top-10 ranking quality is 70.1% of ideal
âœ… Good: Documents are mostly ranked correctly
âœ… Room for improvement: Some relevant docs ranked lower than ideal

Recall@100 = 0.842
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Meaning: Found 84.2% of all relevant documents in top-100
âœ… Good: Most relevant docs are retrieved
âŒ Missing: 15.8% of relevant docs not in top-100

MAP = 0.487
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Meaning: Average precision across all relevant docs is 48.7%
âš  Moderate: Some relevant docs appear late in ranking
â†’ Precision degrades as you go deeper in results

MRR = 1.0
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Meaning: First relevant document is at rank 1
â­ Excellent: User finds answer immediately
\end{lstlisting}

\textbf{Comparison Example}:

\begin{lstlisting}
System Comparison for Query PLAIN-3:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Metric      â”‚ BM25   â”‚ Dense  â”‚ Hybrid â”‚ Winner
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€
NDCG@10     â”‚ 0.653  â”‚ 0.689  â”‚ 0.701  â”‚ Hybrid â­
Recall@100  â”‚ 0.789  â”‚ 0.816  â”‚ 0.842  â”‚ Hybrid â­
MAP         â”‚ 0.423  â”‚ 0.456  â”‚ 0.487  â”‚ Hybrid â­
MRR         â”‚ 0.5    â”‚ 1.0    â”‚ 1.0    â”‚ Dense/Hybrid â­

Analysis:
âœ… Hybrid consistently outperforms
âœ… Dense better at semantic matching (higher MRR)
âœ… BM25 good baseline but limited by lexical matching
\end{lstlisting}

\paragraph{ \textbf{OUTPUT}}

\begin{lstlisting}[language=python]
# Per-query metrics DataFrame
metrics_df = pd.DataFrame([
    {'query_id': 'PLAIN-3', 'ndcg@10': 0.701, 'recall@100': 0.842, 'map': 0.487, 'mrr': 1.0},
    {'query_id': 'PLAIN-4', 'ndcg@10': 0.634, 'recall@100': 0.789, 'map': 0.412, 'mrr': 0.5},
    {'query_id': 'PLAIN-5', 'ndcg@10': 0.723, 'recall@100': 0.868, 'map': 0.523, 'mrr': 1.0},
    ... (323 queries total)
])

# Aggregated summary
summary = {
    'Method': 'Hybrid (RRF)',
    'NDCG@10': 0.682,      # Average across 323 queries
    'Recall@100': 0.815,
    'MAP': 0.456,
    'MRR': 0.734
}

# Comparison table
comparison_df = pd.DataFrame([
    {'Method': 'BM25',    'NDCG@10': 0.612, 'Recall@100': 0.756, 'MAP': 0.398, 'MRR': 0.623},
    {'Method': 'Dense',   'NDCG@10': 0.658, 'Recall@100': 0.789, 'MAP': 0.423, 'MRR': 0.701},
    {'Method': 'Hybrid',  'NDCG@10': 0.682, 'Recall@100': 0.815, 'MAP': 0.456, 'MRR': 0.734}
])
\end{lstlisting}

\textbf{Output Visualization}:

\begin{lstlisting}
Comparison Results:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Method    â”‚ NDCG@10 â”‚ Recall@100 â”‚   MAP   â”‚   MRR   â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
BM25      â”‚  0.612  â”‚   0.756    â”‚  0.398  â”‚  0.623  â”‚
Dense     â”‚  0.658  â”‚   0.789    â”‚  0.423  â”‚  0.701  â”‚
Hybrid    â”‚  0.682â­â”‚   0.815â­  â”‚  0.456â­â”‚  0.734â­â”‚

Improvement over BM25:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dense:  +7.5% NDCG, +4.4% Recall, +6.3% MAP
Hybrid: +11.4% NDCG, +7.8% Recall, +14.6% MAP â­

Key Finding:
âœ… Hybrid retrieval combines strengths of both approaches
âœ… 11.4% improvement in ranking quality (NDCG@10)
âœ… 7.8% improvement in recall (more relevant docs found)
\end{lstlisting}

\subsection{ Summary: Complete Pipeline Flow}

\begin{lstlisting}
User Query: "Do Cholesterol Statin Drugs Cause Breast Cancer?"
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: Data Loading                                         â”‚
â”‚ Input:  corpus.jsonl, queries.jsonl, qrels.tsv                â”‚
â”‚ Output: 3,633 documents, 323 queries, 12,334 judgments        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: Document Chunking                                    â”‚
â”‚ Input:  3,633 full documents                                  â”‚
â”‚ Process: SentenceSplitter (512 tokens, 50 overlap)            â”‚
â”‚ Output: 4,276 nodes/chunks                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                    â†“               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3A: BM25 Storage  â”‚ â”‚ PHASE 3B: Dense Storage â”‚
â”‚ Inverted Index (RAM)    â”‚ â”‚ PostgreSQL+pgvector     â”‚
â”‚ Docstore (Disk)         â”‚ â”‚ HNSW Index              â”‚
â”‚ 50 MB RAM, 2.5 MB Disk  â”‚ â”‚ 8.5 MB Total            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4A: BM25 Search   â”‚ â”‚ PHASE 4B: Dense Search  â”‚
â”‚ Lexical Matching        â”‚ â”‚ Semantic Matching       â”‚
â”‚ BM25 Algorithm          â”‚ â”‚ Cosine Similarity       â”‚
â”‚ Top-100: [MED-10,...]   â”‚ â”‚ Top-100: [MED-10,...]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“               â†“
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ PHASE 4C: Hybrid (RRF)    â”‚
            â”‚ Fuse both rankings        â”‚
            â”‚ RRF: Î±=0.5, k=60          â”‚
            â”‚ Top-10: [MED-10,...]      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ PHASE 5: Evaluation       â”‚
            â”‚ NDCG@10, Recall@100       â”‚
            â”‚ MAP, MRR                  â”‚
            â”‚ Compare all 3 methods     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                    ğŸ“Š Final Results
\end{lstlisting}

\textbf{Key Takeaways}:

\begin{enumerate}
  \item \textbf{Data Flow}: Linear progression from raw data â†’ chunking â†’ indexing â†’ retrieval â†’ evaluation
\end{enumerate}

\begin{enumerate}
  \item \textbf{Storage Strategy}:
\end{enumerate}

\begin{itemize}
  \item BM25: Sparse inverted index (memory-efficient) + persistent docstore
  \item Dense: Full vectors in PostgreSQL (persistent, scalable)
\end{itemize}

\begin{enumerate}
  \item \textbf{Retrieval Strategy}:
\end{enumerate}

\begin{itemize}
  \item BM25: Fast lexical matching, exact word matches
  \item Dense: Semantic understanding, handles synonyms
  \item Hybrid: Best of both worlds via RRF fusion
\end{itemize}

\begin{enumerate}
  \item \textbf{Evaluation}: Multiple metrics capture different aspects of retrieval quality
\end{enumerate}

\begin{enumerate}
  \item \textbf{Performance}: Hybrid > Dense > BM25 (for NFCorpus medical domain)
\end{enumerate}